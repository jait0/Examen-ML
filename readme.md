# Credit Risk Prediction â€“ Machine Learning Pipeline

Este proyecto desarrolla una soluciÃ³n profesional de Machine Learning para predecir la probabilidad de impago (*default*) de clientes solicitantes de crÃ©dito, utilizando informaciÃ³n financiera y crediticia histÃ³rica.

La soluciÃ³n abarca todo el ciclo de vida del modelo bajo la metodologÃ­a **CRISP-DM**:
* AnÃ¡lisis y preparaciÃ³n de datos.
* EliminaciÃ³n de ruido mediante **clustering no supervisado (DBSCAN)**.
* Entrenamiento de un modelo supervisado (**RegresiÃ³n LogÃ­stica**).
* EvaluaciÃ³n del desempeÃ±o y mÃ©tricas de negocio.
* Despliegue mediante una **API REST** escalable.

---

## Estructura del Proyecto

```text
Proyecto root/
â”‚
â”œâ”€â”€ data/                         # Fuentes originales (.parquet)
â”œâ”€â”€ data_output/                  # Datasets procesados para entrenamiento
â”œâ”€â”€ models/                       # Artefactos serializados (.pkl)
â”œâ”€â”€ reports/                      # GrÃ¡ficos y mÃ©tricas de evaluaciÃ³n
â”‚
â”œâ”€â”€ 01_data_understanding/        # CalibraciÃ³n de parÃ¡metros (DBSCAN)
â”œâ”€â”€ 02_data_preparation/          # ETL e integraciÃ³n de fuentes
â”œâ”€â”€ 03_modeling/                  # Entrenamiento del modelo supervisado
â”œâ”€â”€ 04_evaluation/                # Scripts de mÃ©tricas y validaciÃ³n
â”œâ”€â”€ 05_deployment/                # API REST con FastAPI y Schemas
â”‚
â”œâ”€â”€ requirements.txt              # Dependencias del proyecto
â””â”€â”€ README.md                     # DocumentaciÃ³n general
Â´Â´Â´
## Fuentes de Datos
El sistema procesa informaciÃ³n proveniente de tres fuentes clave ubicadas en la carpeta /data:application_.parquet: Datos demogrÃ¡ficos e ingresos del solicitante.bureau.parquet: Historial crediticio externo (BurÃ³).bureau_balance.parquet: Detalle mensual de estados de cuenta externos.ğŸ› ï¸ InstalaciÃ³n y RequisitosPython 3.9+Recomendado: Uso de entorno virtual.Bash# Crear entorno virtual
python -m venv venv

# Activar (Windows)
venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
EjecuciÃ³n del Pipeline (Paso a Paso)1. CalibraciÃ³n de DBSCANIdentifica y calibra los parÃ¡metros para la detecciÃ³n de ruido y outliers.Bashpython 01_data_understanding/dbscan_calibration.py
2. IntegraciÃ³n y Limpieza (ETL)Combina los archivos Parquet, selecciona variables y aplica DBSCAN para eliminar ruido.Bashpython 02_data_preparation/integrate_and_clean.py
3. Entrenamiento del ModeloAplica escalado de variables y entrena la RegresiÃ³n LogÃ­stica.Bashpython 03_modeling/train_model.py
4. EvaluaciÃ³n de DesempeÃ±oGenera la Curva ROC, Matriz de ConfusiÃ³n y reporte de clasificaciÃ³n en la carpeta /reports.Bashpython 04_evaluation/evaluate_model.py
## Despliegue de la API
El sistema utiliza FastAPI para servir el modelo en tiempo real.Levantar el servicio:Bashuvicorn 05_deployment.app:app --reload
API Local: http://127.0.0.1:8000DocumentaciÃ³n Interactiva (Swagger): http://127.0.0.1:8000/docsEndpoint Principal: POST /evaluate_riskRecibe la informaciÃ³n del cliente y retorna el nivel de riesgo.Ejemplo de respuesta:JSON{
  "probabilidad_incumplimiento": "42.35%",
  "decision": "Revisar manualmente"
}
## LÃ³gica de DecisiÃ³n de Negocio
El sistema automatiza la toma de decisiones basada en los siguientes umbrales:Probabilidad (P)DecisiÃ³n$P \geq 70\%$Rechazar$40\% \leq P < 70\%$RevisiÃ³n Manual$P < 40\%$AprobarğŸ’¡ Consideraciones TÃ©cnicasDBSCAN: Se utiliza exclusivamente en la etapa de preparaciÃ³n (limpieza) para mejorar la calidad del entrenamiento, no se requiere en producciÃ³n.Consistencia: El scaler entrenado se reutiliza en la API para garantizar que los datos de entrada sigan la misma distribuciÃ³n.Desacoplamiento: La API es independiente del proceso de entrenamiento, permitiendo actualizaciones del modelo sin afectar el servicio.